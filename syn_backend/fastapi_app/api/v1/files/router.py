import os
import uuid
import asyncio
from datetime import datetime, timezone
from pathlib import Path
from fastapi import APIRouter, Depends, UploadFile, File, Form, HTTPException, status
from typing import Optional
from fastapi_app.db.session import main_db_pool
from fastapi_app.schemas.file import (
    FileResponse, FileListResponse, FileStatsResponse, FileUpdate, FileRenameRequest,
    AIMetadataGenerateRequest, AIMetadataGenerateResponse,
    TranscribeAudioRequest, TranscribeAudioResponse
)
from fastapi_app.schemas.common import Response
from fastapi_app.api.v1.files.services import FileService
from fastapi_app.core.exceptions import NotFoundException, BadRequestException
from fastapi_app.core.logger import logger
from fastapi_app.core.config import settings
from pydantic import BaseModel, Field


router = APIRouter(prefix="/files", tags=["æ–‡ä»¶ç®¡ç†"])


AI_COVER_JOBS: dict[str, dict] = {}
AI_COVER_JOBS_LOCK = asyncio.Lock()


# Dependency: Database connection
async def get_db():
    with main_db_pool.get_connection() as conn:
        yield conn


# Dependency: File service
def get_file_service():
    return FileService()


@router.get(
    "/",
    response_model=FileListResponse,
    summary="è·å–æ–‡ä»¶åˆ—è¡¨",
    description="""
    è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œæ”¯æŒè¿‡æ»¤å’Œåˆ†é¡µã€‚

    æ”¯æŒç­›é€‰æ¡ä»¶:
    - status: æ–‡ä»¶çŠ¶æ€ (pending/published)
    - group: åˆ†ç»„åç§°
    - keyword: æœç´¢å…³é”®è¯ï¼ˆæ”¯æŒæ–‡ä»¶åã€æ ‡é¢˜ã€æè¿°ã€æ ‡ç­¾ï¼‰
    - skip/limit: åˆ†é¡µå‚æ•°ï¼ˆlimit=0è¡¨ç¤ºä¸é™åˆ¶ï¼‰
    """
)
@router.get(
    "",
    response_model=FileListResponse,
    include_in_schema=False
)
async def list_files(
    status: Optional[str] = None,
    group: Optional[str] = None,
    keyword: Optional[str] = None,
    skip: int = 0,
    limit: int = 0,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """è·å–æ–‡ä»¶åˆ—è¡¨"""
    return await service.list_files(db, status=status, group=group, keyword=keyword, skip=skip, limit=limit)


@router.get(
    "/{file_id}",
    response_model=FileResponse,
    summary="è·å–æ–‡ä»¶è¯¦æƒ…",
    description="æ ¹æ®IDè·å–å•ä¸ªæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯"
)
async def get_file(
    file_id: int,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """è·å–æ–‡ä»¶è¯¦æƒ…"""
    file = await service.get_file(db, file_id)
    if not file:
        raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {file_id}")
    return file


class FirstFrameResponse(BaseModel):
    first_frame_path: str = Field(..., description="ç›¸å¯¹è·¯å¾„ï¼Œå¦‚ covers/first_frame_1.png")
    url: str = Field(..., description="å¯ç›´æ¥è®¿é—®çš„ URLï¼ˆ/getFile ä»£ç†ï¼‰")


@router.get(
    "/{file_id}/first-frame",
    response_model=Response,
    summary="ç”Ÿæˆ/è·å–è§†é¢‘é¦–å¸§å°é¢",
)
async def ensure_first_frame(
    file_id: int,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service),
):
    try:
        rel = await service.ensure_first_frame(db, file_id)
        return Response(
            success=True,
            data=FirstFrameResponse(
                first_frame_path=rel,
                url=f"/getFile?filename={rel}",
            ).dict(),
        )
    except NotFoundException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except BadRequestException as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"ç”Ÿæˆé¦–å¸§å°é¢å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


class AICoverRequest(BaseModel):
    platform_name: str = Field(default="å…¨å¹³å°", description="å¹³å°åç§°ï¼Œç”¨äºè°ƒæ€§ prompt")
    aspect_ratio: str = Field(default="3:4", description="3:4 / 4ï¼š3")
    style_hint: str = Field(default="", description="é¢å¤–é£æ ¼è¡¥å……ï¼ˆå¯é€‰ï¼‰")
    prompt: str = Field(default="", description="è¦†ç›–è‡ªåŠ¨ promptï¼ˆå¯é€‰ï¼‰")


async def _run_ai_cover_job(
    job_id: str,
    file_id: int,
    *,
    platform_name: str,
    aspect_ratio: str,
    style_hint: str,
    prompt: str,
    ref_image_bytes: Optional[bytes],
):
    async with AI_COVER_JOBS_LOCK:
        job = AI_COVER_JOBS.get(job_id)
        if job is None:
            return
        job["status"] = "running"
        job["updated_at"] = datetime.now(timezone.utc).isoformat()

    try:
        service = FileService()
        with main_db_pool.get_connection() as conn:
            data = await service.generate_ai_cover(
                conn,
                file_id,
                platform_name=platform_name,
                aspect_ratio=aspect_ratio,
                style_hint=style_hint,
                prompt_override=prompt,
                image_bytes_override=ref_image_bytes,
            )

        async with AI_COVER_JOBS_LOCK:
            job = AI_COVER_JOBS.get(job_id)
            if job is None:
                return
            job["status"] = "succeeded"
            job["cover_path"] = data.get("cover_path") or ""
            job["first_frame_path"] = data.get("first_frame_path") or ""
            job["prompt"] = data.get("prompt") or ""
            job["raw_url"] = data.get("raw_url") or ""
            job["updated_at"] = datetime.now(timezone.utc).isoformat()
    except Exception as e:
        async with AI_COVER_JOBS_LOCK:
            job = AI_COVER_JOBS.get(job_id)
            if job is None:
                return
            job["status"] = "failed"
            job["error"] = str(e)
            job["updated_at"] = datetime.now(timezone.utc).isoformat()
        logger.error(f"AI cover job failed (job_id={job_id} file_id={file_id}): {e}", exc_info=True)


@router.post(
    "/{file_id}/ai-cover",
    response_model=Response,
    summary="åŸºäºé¦–å¸§ AI ç”Ÿæˆç»Ÿä¸€å°é¢ï¼ˆç¡…åŸºæµåŠ¨ï¼‰",
)
async def generate_ai_cover(
    file_id: int,
    req: AICoverRequest,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service),
):
    try:
        data = await service.generate_ai_cover(
            db,
            file_id,
            platform_name=req.platform_name,
            aspect_ratio=req.aspect_ratio,
            style_hint=req.style_hint,
            prompt_override=req.prompt,
        )
        cover_path = data.get("cover_path") or ""
        first_frame_path = data.get("first_frame_path") or ""
        return Response(
            success=True,
            data={
                **data,
                "cover_url": f"/getFile?filename={cover_path}" if cover_path else "",
                "first_frame_url": f"/getFile?filename={first_frame_path}" if first_frame_path else "",
            },
        )
    except NotFoundException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except BadRequestException as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"AI ç”Ÿæˆå°é¢å¤±è´¥: {e!r}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post(
    "/{file_id}/ai-cover-job",
    response_model=Response,
    summary="å¼‚æ­¥ AI ç”Ÿæˆå°é¢ï¼ˆä¸é˜»å¡å‰ç«¯ï¼‰",
)
async def start_ai_cover_job(
    file_id: int,
    platform_name: str = Form("å…¨å¹³å°"),
    aspect_ratio: str = Form("3:4"),
    style_hint: str = Form(""),
    prompt: str = Form(""),
    ref_image: Optional[UploadFile] = File(None, description="å‚è€ƒå›¾ï¼ˆå¯é€‰ï¼Œç”¨äºå›¾ç”Ÿå›¾ï¼‰"),
):
    ref_bytes = await ref_image.read() if ref_image else None

    job_id = str(uuid.uuid4())
    now = datetime.now(timezone.utc).isoformat()
    async with AI_COVER_JOBS_LOCK:
        AI_COVER_JOBS[job_id] = {
            "job_id": job_id,
            "file_id": file_id,
            "status": "pending",
            "created_at": now,
            "updated_at": now,
            "cover_path": "",
            "first_frame_path": "",
            "prompt": "",
            "raw_url": "",
            "error": "",
        }

    asyncio.create_task(
        _run_ai_cover_job(
            job_id,
            file_id,
            platform_name=platform_name,
            aspect_ratio=aspect_ratio,
            style_hint=style_hint,
            prompt=prompt,
            ref_image_bytes=ref_bytes,
        )
    )

    return Response(success=True, data={"job_id": job_id, "status": "pending"})


@router.get(
    "/ai-cover-jobs/{job_id}",
    response_model=Response,
    summary="æŸ¥è¯¢å¼‚æ­¥ AI å°é¢ä»»åŠ¡çŠ¶æ€",
)
async def get_ai_cover_job(job_id: str):
    async with AI_COVER_JOBS_LOCK:
        job = AI_COVER_JOBS.get(job_id)
        if not job:
            raise HTTPException(status_code=404, detail="job not found")
        data = dict(job)

    cover_path = data.get("cover_path") or ""
    first_frame_path = data.get("first_frame_path") or ""
    data["cover_url"] = f"/getFile?filename={cover_path}" if cover_path else ""
    data["first_frame_url"] = f"/getFile?filename={first_frame_path}" if first_frame_path else ""
    return Response(success=True, data=data)


@router.post(
    "/upload",
    response_model=Response,
    summary="ç®€å•æ–‡ä»¶ä¸Šä¼ ",
    description="""
    ä»…ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨ï¼Œä¸åˆ›å»ºæ•°æ®åº“è®°å½•ã€‚

    é™åˆ¶:
    - æœ€å¤§æ–‡ä»¶å¤§å°: 160MB
    - æ”¯æŒæ ¼å¼: mp4, mov, avi, mkvç­‰è§†é¢‘æ ¼å¼
    """
)
async def upload_file(
    file: UploadFile = File(..., description="è¦ä¸Šä¼ çš„æ–‡ä»¶"),
    service: FileService = Depends(get_file_service)
):
    """ç®€å•æ–‡ä»¶ä¸Šä¼ """
    try:
        # Validate file
        if not file.filename:
            raise BadRequestException("æ–‡ä»¶åä¸èƒ½ä¸ºç©º")

        # Calculate file size
        content = await file.read()
        filesize_mb = len(content) / (1024 * 1024)

        # Validate size (160MB limit)
        if filesize_mb > 160:
            raise BadRequestException(f"æ–‡ä»¶è¿‡å¤§: {filesize_mb:.2f}MBï¼Œé™åˆ¶160MB")

        # Validate disk space
        if not service.validate_disk_space(filesize_mb):
            raise BadRequestException("ç£ç›˜ç©ºé—´ä¸è¶³")

        # Generate unique filename
        ext = Path(file.filename).suffix
        unique_filename = f"{uuid.uuid4()}{ext}"
        file_path = Path(settings.VIDEO_FILES_DIR) / unique_filename

        # Save file
        with open(file_path, "wb") as f:
            f.write(content)

        logger.info(f"File uploaded: {file.filename} -> {file_path} ({filesize_mb:.2f}MB)")

        return Response(
            success=True,
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            data={
                "filename": file.filename,
                "saved_path": str(file_path),
                "size_mb": round(filesize_mb, 2)
            }
        )

    except Exception as e:
        logger.error(f"File upload error: {e}")
        if isinstance(e, (BadRequestException, NotFoundException)):
            raise
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")


@router.post(
    "/upload-save",
    response_model=Response,
    summary="ä¸Šä¼ å¹¶ä¿å­˜è®°å½•",
    description="""
    ä¸Šä¼ æ–‡ä»¶å¹¶åœ¨æ•°æ®åº“ä¸­åˆ›å»ºè®°å½•ã€‚

    æ”¯æŒå‚æ•°:
    - file: æ–‡ä»¶ (å¿…éœ€)
    - note: å¤‡æ³¨ä¿¡æ¯ (å¯é€‰)
    - group: åˆ†ç»„åç§° (å¯é€‰)
    """
)
async def upload_and_save(
    file: UploadFile = File(..., description="è¦ä¸Šä¼ çš„æ–‡ä»¶"),
    filename: Optional[str] = Form(None, description="è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆä»…ç”¨äºæ˜¾ç¤ºï¼Œä¸æ”¹ç£ç›˜æ–‡ä»¶åï¼‰"),
    note: Optional[str] = Form(None, description="å¤‡æ³¨ä¿¡æ¯"),
    group: Optional[str] = Form(None, description="åˆ†ç»„åç§°"),
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """ä¸Šä¼ æ–‡ä»¶å¹¶ä¿å­˜è®°å½•"""
    try:
        # Validate file
        if not file.filename:
            raise BadRequestException("æ–‡ä»¶åä¸èƒ½ä¸ºç©º")

        # Calculate file size
        content = await file.read()
        filesize_mb = len(content) / (1024 * 1024)

        # Validate size (160MB limit)
        if filesize_mb > 160:
            raise BadRequestException(f"æ–‡ä»¶è¿‡å¤§: {filesize_mb:.2f}MBï¼Œé™åˆ¶160MB")

        # Validate disk space
        if not service.validate_disk_space(filesize_mb):
            raise BadRequestException("ç£ç›˜ç©ºé—´ä¸è¶³")

        # Optional display filename (sanitize, keep extension)
        display_filename = file.filename
        if filename:
            raw_custom = filename.strip()
            if raw_custom:
                safe_custom = Path(raw_custom).name  # prevent path traversal
                if not Path(safe_custom).suffix:
                    safe_custom = f"{safe_custom}{Path(file.filename).suffix}"
                display_filename = safe_custom

        # Generate unique filename
        ext = Path(file.filename).suffix
        unique_filename = f"{uuid.uuid4()}{ext}"
        file_path = Path(settings.VIDEO_FILES_DIR) / unique_filename

        # Save file
        with open(file_path, "wb") as f:
            f.write(content)

        # Save database record - âš ï¸ åªå­˜å‚¨æ–‡ä»¶åï¼ˆç›¸å¯¹è·¯å¾„ï¼‰ï¼Œä¾¿äºè·¨æœºå™¨è¿ç§»
        file_id = await service.save_file_record(
            db,
            filename=display_filename,
            file_path=unique_filename,  # åªå­˜å‚¨æ–‡ä»¶åï¼Œå¦‚ "abc123.mp4"
            filesize_mb=filesize_mb,
            note=note,
            group_name=group
        )

        # ğŸ†• è‡ªåŠ¨ç”Ÿæˆé¦–å¸§é¢„è§ˆå›¾ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡å“åº”ï¼‰
        cover_path = None
        try:
            cover_path = await service.ensure_first_frame(db, file_id)
            logger.info(f"é¦–å¸§é¢„è§ˆå›¾å·²ç”Ÿæˆ: {cover_path}")
        except Exception as e:
            logger.warning(f"ç”Ÿæˆé¦–å¸§é¢„è§ˆå›¾å¤±è´¥ï¼ˆä¸å½±å“ä¸Šä¼ ï¼‰: {e}")

        logger.info(
            f"File uploaded and saved: {file.filename} -> {file_path} "
            f"({filesize_mb:.2f}MB, ID: {file_id})"
        )

        return Response(
            success=True,
            message="æ–‡ä»¶ä¸Šä¼ å¹¶ä¿å­˜æˆåŠŸ",
            data={
                "id": file_id,
                "filename": display_filename,
                "file_path": str(file_path),
                "size_mb": round(filesize_mb, 2),
                "note": note,
                "group_name": group,
                "cover_path": cover_path  # ğŸ†• è¿”å›é¦–å¸§é¢„è§ˆå›¾è·¯å¾„
            }
        )

    except Exception as e:
        # Cleanup file if database save fails
        if 'file_path' in locals() and os.path.exists(file_path):
            os.remove(file_path)

        logger.error(f"Upload and save error: {e}")
        if isinstance(e, (BadRequestException, NotFoundException)):
            raise
        raise HTTPException(status_code=500, detail=f"æ“ä½œå¤±è´¥: {str(e)}")


@router.patch(
    "/{file_id}/rename",
    response_model=Response,
    summary="é‡å‘½åæ–‡ä»¶",
    description="""
    é‡å‘½åæ–‡ä»¶ï¼ˆåŒæ­¥ä¿®æ”¹æ•°æ®åº“å’Œç£ç›˜æ–‡ä»¶åï¼‰

    åŠŸèƒ½ï¼š
    - ä¿®æ”¹æ•°æ®åº“ä¸­çš„ filename å’Œ file_path
    - åŒæ­¥ä¿®æ”¹ç£ç›˜ä¸Šçš„ç‰©ç†æ–‡ä»¶å
    - æ”¯æŒé€‰æ‹©æ˜¯å¦åŒæ­¥ä¿®æ”¹ç£ç›˜æ–‡ä»¶
    - è‡ªåŠ¨æ£€æŸ¥æ–‡ä»¶åå†²çª

    å‚æ•°ï¼š
    - new_filename: æ–°æ–‡ä»¶åï¼ˆå¿…é¡»åŒ…å«æ‰©å±•åï¼‰
    - update_disk_file: æ˜¯å¦åŒæ­¥ä¿®æ”¹ç£ç›˜æ–‡ä»¶ï¼ˆé»˜è®¤ trueï¼‰
    """
)
async def rename_file(
    file_id: int,
    rename_request: FileRenameRequest,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """é‡å‘½åæ–‡ä»¶ï¼ˆåŒæ­¥æ•°æ®åº“å’Œç£ç›˜ï¼‰"""
    try:
        success = await service.rename_file(
            db,
            file_id,
            rename_request.new_filename,
            rename_request.update_disk_file
        )

        if not success:
            raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {file_id}")

        return Response(
            success=True,
            message="æ–‡ä»¶é‡å‘½åæˆåŠŸ",
            data={
                "id": file_id,
                "new_filename": rename_request.new_filename,
                "disk_updated": rename_request.update_disk_file
            }
        )

    except (NotFoundException, BadRequestException):
        raise
    except Exception as e:
        logger.error(f"Rename file error: {e}")
        raise HTTPException(status_code=500, detail=f"é‡å‘½åå¤±è´¥: {str(e)}")


@router.patch(
    "/{file_id}",
    response_model=Response,
    summary="æ›´æ–°æ–‡ä»¶å…ƒæ•°æ®",
    description="æ›´æ–°æ–‡ä»¶çš„å¤‡æ³¨ã€åˆ†ç»„æˆ–çŠ¶æ€ä¿¡æ¯"
)
async def update_file_metadata(
    file_id: int,
    update_data: FileUpdate,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """æ›´æ–°æ–‡ä»¶å…ƒæ•°æ®"""
    success = await service.update_file(db, file_id, update_data)
    if not success:
        raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {file_id}")

    return Response(
        success=True,
        message="æ–‡ä»¶ä¿¡æ¯æ›´æ–°æˆåŠŸ",
        data={"id": file_id}
    )


class GroupRenameRequest(BaseModel):
    from_name: str = Field(..., description="åŸåˆ†ç»„å")
    to_name: str = Field(..., description="æ–°åˆ†ç»„å")


class GroupDeleteRequest(BaseModel):
    name: str = Field(..., description="å¾…åˆ é™¤åˆ†ç»„å")


@router.get(
    "/groups",
    response_model=Response,
    summary="è·å–åˆ†ç»„åˆ—è¡¨",
)
async def list_groups(
    db=Depends(get_db),
    service: FileService = Depends(get_file_service),
):
    groups = await service.list_groups(db)
    return Response(success=True, data={"groups": groups})


@router.post(
    "/groups/rename",
    response_model=Response,
    summary="é‡å‘½ååˆ†ç»„ï¼ˆæ‰¹é‡ï¼‰",
)
async def rename_group(
    req: GroupRenameRequest,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service),
):
    updated = await service.rename_group(db, req.from_name, req.to_name)
    return Response(success=True, message="åˆ†ç»„é‡å‘½åæˆåŠŸ", data={"updated": updated})


@router.post(
    "/groups/delete",
    response_model=Response,
    summary="åˆ é™¤åˆ†ç»„ï¼ˆæ‰¹é‡ï¼‰",
)
async def delete_group(
    req: GroupDeleteRequest,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service),
):
    updated = await service.delete_group(db, req.name)
    return Response(success=True, message="åˆ†ç»„å·²åˆ é™¤", data={"updated": updated})


@router.post(
    "/sync",
    response_model=Response,
    summary="åŒæ­¥ç£ç›˜ç´ æåˆ°åº“",
    description="æ‰«æè§†é¢‘ç›®å½•ï¼Œå°†ç¼ºå¤±çš„æ–‡ä»¶å…¥åº“ã€‚ä¸ä¼šåˆ é™¤å·²æœ‰è®°å½•ã€‚"
)
async def sync_files(
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """
    æ‰«æ settings.VIDEO_FILES_DIRï¼Œå°†æœªå…¥åº“çš„æ–‡ä»¶è¡¥å……åˆ° file_recordsã€‚
    è¿”å›æ‰«ææ•°é‡ä¸æ–°å¢æ•°é‡ã€‚
    """
    scanned = 0
    added = 0
    dedup_deleted = 0
    normalized_paths = 0

    video_dir = Path(settings.VIDEO_FILES_DIR)
    video_dir.mkdir(parents=True, exist_ok=True)

    video_extensions = {".mp4", ".mov", ".avi", ".mkv", ".flv", ".wmv", ".webm", ".m4v"}

    cursor = db.cursor()

    cursor.execute("""
        SELECT id, filename, file_path
        FROM file_records
        WHERE file_path IS NOT NULL AND file_path != ''
        ORDER BY id ASC
    """)
    rows = [dict(r) for r in cursor.fetchall()]

    # Normalize legacy absolute `file_path` and deduplicate sync artifacts.
    for r in rows:
        file_id = r.get("id")
        stored = r.get("file_path")
        if not file_id or not stored:
            continue
        try:
            p = Path(str(stored))
            if p.is_absolute() and video_dir in p.parents and (video_dir / p.name).exists():
                cursor.execute("UPDATE file_records SET file_path = ? WHERE id = ?", (p.name, file_id))
                normalized_paths += 1
                r["file_path"] = p.name
        except Exception:
            continue

    by_key: dict[str, list[dict]] = {}
    for r in rows:
        key = Path(str(r.get("file_path") or "")).name
        if not key:
            continue
        by_key.setdefault(key, []).append(r)

    delete_ids: list[int] = []
    for key, group_rows in by_key.items():
        if len(group_rows) <= 1:
            continue
        keep = next((x for x in group_rows if (x.get("filename") or "") != key), group_rows[0])
        for x in group_rows:
            if x.get("id") != keep.get("id") and (x.get("filename") or "") == key:
                delete_ids.append(int(x["id"]))

    if normalized_paths or delete_ids:
        if delete_ids:
            cursor.executemany("DELETE FROM file_records WHERE id = ?", [(i,) for i in delete_ids])
            dedup_deleted = len(delete_ids)
        db.commit()

    existing_paths = set(by_key.keys())

    for file_path in video_dir.iterdir():
        if not file_path.is_file():
            continue
        if file_path.name.startswith("."):
            continue
        if file_path.suffix.lower() not in video_extensions:
            continue

        scanned += 1
        if file_path.name in existing_paths:
            continue

        filesize_mb = file_path.stat().st_size / (1024 * 1024)
        await service.save_file_record(
            db=db,
            filename=file_path.name,
            file_path=file_path.name,  # åªå­˜å‚¨æ–‡ä»¶åï¼Œä¾¿äºè·¨æœºå™¨è¿ç§»
            filesize_mb=round(filesize_mb, 2),
            note=None,
            group_name=None,
        )
        added += 1
        existing_paths.add(file_path.name)

    return Response(
        success=True,
        message="åŒæ­¥å®Œæˆ",
        data={
            "scanned": scanned,
            "added": added,
            "dedup_deleted": dedup_deleted,
            "normalized_paths": normalized_paths,
        }
    )


@router.delete(
    "/{file_id}",
    response_model=Response,
    summary="åˆ é™¤æ–‡ä»¶",
    description="åˆ é™¤æ–‡ä»¶è®°å½•åŠç£ç›˜æ–‡ä»¶ï¼ˆåŸå­æ“ä½œï¼‰"
)
async def delete_file(
    file_id: int,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """åˆ é™¤æ–‡ä»¶"""
    success = await service.delete_file(db, file_id)
    if not success:
        raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {file_id}")

    return Response(
        success=True,
        message="æ–‡ä»¶åˆ é™¤æˆåŠŸ",
        data={"id": file_id}
    )


class BatchDeleteRequest(BaseModel):
    """æ‰¹é‡åˆ é™¤è¯·æ±‚"""
    file_ids: list[int] = Field(..., description="è¦åˆ é™¤çš„æ–‡ä»¶IDåˆ—è¡¨", min_items=1)


class BatchDeleteResponse(BaseModel):
    """æ‰¹é‡åˆ é™¤å“åº”"""
    success_count: int = Field(..., description="æˆåŠŸåˆ é™¤çš„æ–‡ä»¶æ•°")
    failed_count: int = Field(..., description="åˆ é™¤å¤±è´¥çš„æ–‡ä»¶æ•°")
    failed_ids: list[int] = Field(default_factory=list, description="åˆ é™¤å¤±è´¥çš„æ–‡ä»¶IDåˆ—è¡¨")


@router.post(
    "/batch-delete",
    response_model=Response,
    summary="æ‰¹é‡åˆ é™¤æ–‡ä»¶",
    description="""
    æ‰¹é‡åˆ é™¤å¤šä¸ªæ–‡ä»¶ï¼Œå•æ¬¡è¯·æ±‚å®Œæˆæ‰€æœ‰åˆ é™¤æ“ä½œã€‚

    ä¼˜åŠ¿ï¼š
    - å•æ¬¡HTTPè¯·æ±‚ï¼Œé¿å…å¹¶å‘ç«äº‰
    - æ‰¹é‡æ•°æ®åº“æ“ä½œï¼Œæ€§èƒ½æ›´é«˜
    - æ‰¹é‡æ–‡ä»¶åˆ é™¤ï¼Œå‡å°‘I/Oé˜»å¡
    - æ”¯æŒéƒ¨åˆ†æˆåŠŸï¼Œè¿”å›å¤±è´¥åˆ—è¡¨

    å‚æ•°ï¼š
    - file_ids: è¦åˆ é™¤çš„æ–‡ä»¶IDåˆ—è¡¨
    """
)
async def batch_delete_files(
    request: BatchDeleteRequest,
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """æ‰¹é‡åˆ é™¤æ–‡ä»¶"""
    try:
        result = await service.batch_delete_files(db, request.file_ids)

        success_count = result["success_count"]
        failed_count = result["failed_count"]
        failed_ids = result["failed_ids"]

        message = f"æ‰¹é‡åˆ é™¤å®Œæˆ: æˆåŠŸ{success_count}ä¸ª, å¤±è´¥{failed_count}ä¸ª"
        if failed_count > 0:
            message += f"ï¼ˆå¤±è´¥ID: {failed_ids}ï¼‰"

        return Response(
            success=True,
            message=message,
            data=BatchDeleteResponse(
                success_count=success_count,
                failed_count=failed_count,
                failed_ids=failed_ids
            ).dict()
        )

    except Exception as e:
        logger.error(f"Batch delete error: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ é™¤å¤±è´¥: {str(e)}")


@router.get(
    "/stats/summary",
    response_model=FileStatsResponse,
    summary="æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯",
    description="è·å–æ–‡ä»¶æ€»æ•°ã€çŠ¶æ€åˆ†å¸ƒã€å­˜å‚¨å ç”¨ç­‰ç»Ÿè®¡ä¿¡æ¯"
)
async def get_file_stats(
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """è·å–æ–‡ä»¶ç»Ÿè®¡"""
    return await service.get_stats(db)


@router.post(
    "/sync/legacy",
    response_model=Response,
    summary="åŒæ­¥ç£ç›˜æ–‡ä»¶",
    description="""
    æ‰«æ videoFile ç›®å½•å¹¶å°†æœªå…¥åº“çš„æ–‡ä»¶åŒæ­¥åˆ°æ•°æ®åº“ã€‚
    
    åŠŸèƒ½:
    - è‡ªåŠ¨æ‰«æ videoFile ç›®å½•
    - è¯†åˆ«è§†é¢‘æ–‡ä»¶ (mp4, mov, avi, mkv ç­‰)
    - å°†æœªå…¥åº“çš„æ–‡ä»¶æ·»åŠ åˆ°æ•°æ®åº“
    - è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
    - è¿”å›è¯¦ç»†çš„åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
    """
)
async def sync_files(
    db=Depends(get_db),
    service: FileService = Depends(get_file_service)
):
    """åŒæ­¥ç£ç›˜æ–‡ä»¶åˆ°æ•°æ®åº“"""
    try:
        stats = await service.sync_files_from_disk(db)
        
        return Response(
            success=True,
            message=f"åŒæ­¥å®Œæˆ: æ‰«æ{stats['scanned']}ä¸ªæ–‡ä»¶ï¼Œæ–°å¢{stats['added']}ä¸ªï¼Œè·³è¿‡{stats['skipped']}ä¸ª",
            data=stats
        )
    except Exception as e:
        logger.error(f"File sync error: {e}")
        raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}")


# ============================================
# AIå†…å®¹ç”Ÿæˆå’Œç»‘å®š
# ============================================

from pydantic import BaseModel
from typing import List

class AIContentUpdate(BaseModel):
    """AIç”Ÿæˆå†…å®¹æ›´æ–°è¯·æ±‚"""
    ai_title: Optional[str] = None
    ai_description: Optional[str] = None
    ai_tags: Optional[List[str]] = None


@router.patch(
    "/{file_id}/ai-content",
    response_model=Response,
    summary="æ›´æ–°ç´ æçš„AIç”Ÿæˆå†…å®¹",
    description="""
    æ›´æ–°ç´ æçš„AIç”Ÿæˆæ ‡é¢˜ã€æè¿°å’Œæ ‡ç­¾ã€‚

    AIç”Ÿæˆçš„å†…å®¹ä¼šæ°¸ä¹…ç»‘å®šåˆ°ç´ æï¼Œå¯åœ¨å‘å¸ƒæ—¶å¿«é€Ÿä½¿ç”¨ã€‚
    æ”¯æŒéƒ¨åˆ†æ›´æ–°ï¼Œæœªä¼ é€’çš„å­—æ®µä¿æŒåŸå€¼ä¸å˜ã€‚

    å‚æ•°:
    - ai_title: AIç”Ÿæˆçš„æ ‡é¢˜
    - ai_description: AIç”Ÿæˆçš„æè¿°
    - ai_tags: AIç”Ÿæˆçš„æ ‡ç­¾åˆ—è¡¨
    """
)
async def update_ai_content(
    file_id: int,
    content: AIContentUpdate,
    db=Depends(get_db)
):
    """æ›´æ–°AIç”Ÿæˆå†…å®¹"""
    try:
        cursor = db.cursor()

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT id FROM file_records WHERE id = ?", (file_id,))
        if not cursor.fetchone():
            raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {file_id}")

        # æ£€æŸ¥å¹¶æ·»åŠ AIå†…å®¹å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        cursor.execute("PRAGMA table_info(file_records)")
        columns = [row[1] for row in cursor.fetchall()]

        if 'ai_title' not in columns:
            cursor.execute("ALTER TABLE file_records ADD COLUMN ai_title TEXT")
        if 'ai_description' not in columns:
            cursor.execute("ALTER TABLE file_records ADD COLUMN ai_description TEXT")
        if 'ai_tags' not in columns:
            cursor.execute("ALTER TABLE file_records ADD COLUMN ai_tags TEXT")
        if 'ai_generated_at' not in columns:
            cursor.execute("ALTER TABLE file_records ADD COLUMN ai_generated_at TIMESTAMP")

        # æ„å»ºæ›´æ–°SQL
        updates = []
        params = []

        if content.ai_title is not None:
            updates.append("ai_title = ?")
            params.append(content.ai_title)

        if content.ai_description is not None:
            updates.append("ai_description = ?")
            params.append(content.ai_description)

        if content.ai_tags is not None:
            import json
            updates.append("ai_tags = ?")
            params.append(json.dumps(content.ai_tags, ensure_ascii=False))

        if updates:
            updates.append("ai_generated_at = CURRENT_TIMESTAMP")
            params.append(file_id)

            sql = f"UPDATE file_records SET {', '.join(updates)} WHERE id = ?"
            cursor.execute(sql, params)
            db.commit()

            logger.info(f"AI content updated for file {file_id}")

        return Response(
            success=True,
            message="AIå†…å®¹å·²ä¿å­˜",
            data={
                "id": file_id,
                "ai_title": content.ai_title,
                "ai_description": content.ai_description,
                "ai_tags": content.ai_tags
            }
        )

    except NotFoundException:
        raise
    except Exception as e:
        logger.error(f"Update AI content error: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±è´¥: {str(e)}")


@router.get(
    "/{file_id}/ai-content",
    response_model=Response,
    summary="è·å–ç´ æçš„AIç”Ÿæˆå†…å®¹",
    description="è·å–æŒ‡å®šç´ æçš„AIç”Ÿæˆæ ‡é¢˜ã€æè¿°å’Œæ ‡ç­¾"
)
async def get_ai_content(
    file_id: int,
    db=Depends(get_db)
):
    """è·å–AIç”Ÿæˆå†…å®¹"""
    try:
        cursor = db.cursor()

        # æ£€æŸ¥AIå†…å®¹å­—æ®µæ˜¯å¦å­˜åœ¨
        cursor.execute("PRAGMA table_info(file_records)")
        columns = [row[1] for row in cursor.fetchall()]

        has_ai_fields = all(col in columns for col in ['ai_title', 'ai_description', 'ai_tags', 'ai_generated_at'])

        if not has_ai_fields:
            # å­—æ®µä¸å­˜åœ¨ï¼Œè¿”å›ç©ºå†…å®¹
            return Response(
                success=True,
                message="è¯¥ç´ ææš‚æ— AIç”Ÿæˆå†…å®¹",
                data={
                    "id": file_id,
                    "ai_title": None,
                    "ai_description": None,
                    "ai_tags": [],
                    "ai_generated_at": None
                }
            )

        # æŸ¥è¯¢AIå†…å®¹
        cursor.execute("""
            SELECT ai_title, ai_description, ai_tags, ai_generated_at
            FROM file_records
            WHERE id = ?
        """, (file_id,))

        row = cursor.fetchone()
        if not row:
            raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {file_id}")

        import json
        ai_tags = json.loads(row[2]) if row[2] else []

        return Response(
            success=True,
            message="è·å–AIå†…å®¹æˆåŠŸ",
            data={
                "id": file_id,
                "ai_title": row[0],
                "ai_description": row[1],
                "ai_tags": ai_tags,
                "ai_generated_at": row[3]
            }
        )

    except NotFoundException:
        raise
    except Exception as e:
        logger.error(f"Get AI content error: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.delete(
    "/{file_id}/ai-content",
    response_model=Response,
    summary="æ¸…é™¤ç´ æçš„AIç”Ÿæˆå†…å®¹",
    description="æ¸…é™¤æŒ‡å®šç´ æçš„æ‰€æœ‰AIç”Ÿæˆå†…å®¹"
)
async def clear_ai_content(
    file_id: int,
    db=Depends(get_db)
):
    """æ¸…é™¤AIç”Ÿæˆå†…å®¹"""
    try:
        cursor = db.cursor()

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT id FROM file_records WHERE id = ?", (file_id,))
        if not cursor.fetchone():
            raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {file_id}")

        # æ¸…é™¤AIå†…å®¹
        cursor.execute("""
            UPDATE file_records
            SET ai_title = NULL,
                ai_description = NULL,
                ai_tags = NULL,
                ai_generated_at = NULL
            WHERE id = ?
        """, (file_id,))
        db.commit()

        return Response(
            success=True,
            message="AIå†…å®¹å·²æ¸…é™¤",
            data={"id": file_id}
        )

    except NotFoundException:
        raise
    except Exception as e:
        logger.error(f"Clear AI content error: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…é™¤å¤±è´¥: {str(e)}")


# ============================================
# AIæ‰¹é‡å…ƒæ•°æ®ç”Ÿæˆ
# ============================================

@router.post(
    "/batch-generate-metadata",
    response_model=AIMetadataGenerateResponse,
    summary="æ‰¹é‡ç”ŸæˆAIå…ƒæ•°æ®",
    description="""
    ä¸ºå¤šä¸ªè§†é¢‘æ–‡ä»¶æ‰¹é‡ç”ŸæˆAIæ ‡é¢˜ã€æè¿°å’Œæ ‡ç­¾ã€‚

    åŠŸèƒ½ï¼š
    - è‡ªåŠ¨åˆ†æè§†é¢‘æ–‡ä»¶å
    - æ™ºèƒ½ç”Ÿæˆæ ‡é¢˜ã€æè¿°å’Œæ ‡ç­¾
    - æ”¯æŒå¼ºåˆ¶é‡æ–°ç”Ÿæˆ
    - è¿”å›è¯¦ç»†çš„ç”Ÿæˆç»“æœ

    å‚æ•°ï¼š
    - file_ids: æ–‡ä»¶IDåˆ—è¡¨
    - force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼ˆå³ä½¿å·²æœ‰AIå†…å®¹ï¼‰
    """
)
async def batch_generate_ai_metadata(
    request: AIMetadataGenerateRequest,
    db=Depends(get_db)
):
    """æ‰¹é‡ç”ŸæˆAIå…ƒæ•°æ®"""
    try:
        from ai_service.model_manager import get_model_manager

        cursor = db.cursor()
        results = []
        success_count = 0
        failed_count = 0

        # è·å–AIæ¨¡å‹ç®¡ç†å™¨
        model_manager = get_model_manager()

        for file_id in request.file_ids:
            try:
                # æŸ¥è¯¢æ–‡ä»¶ä¿¡æ¯
                cursor.execute("""
                    SELECT id, filename, file_path, title, tags, ai_title, ai_description, ai_tags
                    FROM file_records
                    WHERE id = ?
                """, (file_id,))

                row = cursor.fetchone()
                if not row:
                    results.append({
                        "file_id": file_id,
                        "status": "failed",
                        "error": "æ–‡ä»¶ä¸å­˜åœ¨"
                    })
                    failed_count += 1
                    continue

                (
                    file_id_val,
                    filename,
                    file_path,
                    user_title,
                    user_tags,
                    existing_ai_title,
                    existing_ai_desc,
                    existing_ai_tags,
                ) = row

                # æ£€æŸ¥æ˜¯å¦å·²æœ‰AIå†…å®¹ä¸”ä¸å¼ºåˆ¶é‡æ–°ç”Ÿæˆ
                if not request.force_regenerate and existing_ai_title:
                    results.append({
                        "file_id": file_id,
                        "status": "skipped",
                        "message": "å·²æœ‰AIå†…å®¹ï¼Œè·³è¿‡ç”Ÿæˆ"
                    })
                    continue

                # ä½¿ç”¨AIç”Ÿæˆå…ƒæ•°æ®
                prompt = f"""è¯·åŸºäºã€Œæ–‡ä»¶åã€ä»¥åŠã€Œç”¨æˆ·å·²æœ‰æ ‡é¢˜/æ ‡ç­¾ã€ï¼Œç”Ÿæˆé€‚åˆçŸ­è§†é¢‘å¹³å°çš„ AI æ ‡é¢˜ã€æè¿°å’Œæ ‡ç­¾ï¼Œå¹¶å°½é‡åšâ€œæ”¹ç¼–ä¼˜åŒ–â€è€Œä¸æ˜¯å®Œå…¨é‡å†™ã€‚

è¾“å…¥ï¼š
- æ–‡ä»¶åï¼š{filename}
- ç”¨æˆ·æ ‡é¢˜ï¼ˆå¯ä¸ºç©ºï¼‰ï¼š{user_title or ""}
- ç”¨æˆ·æ ‡ç­¾ï¼ˆå¯ä¸ºç©ºï¼Œå¯èƒ½ä¸º JSON æ•°ç»„/ç©ºæ ¼åˆ†éš”/é€—å·åˆ†éš”ï¼‰ï¼š{user_tags or ""}

è¾“å‡ºè¦æ±‚ï¼ˆä¸¥æ ¼ JSONï¼Œç¦æ­¢ markdown/è§£é‡Š/å¤šä½™æ–‡æœ¬ï¼‰ï¼š
{{
  "title": "æ ‡é¢˜ï¼ˆ<=30å­—ï¼Œä¸­æ–‡ä¼˜å…ˆï¼‰",
  "description": "æè¿°ï¼ˆ50-120å­—ï¼Œä¸­æ–‡ä¼˜å…ˆï¼‰",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"]
}}

çº¦æŸï¼š
1) å¦‚æœç”¨æˆ·æ ‡é¢˜/æ ‡ç­¾å­˜åœ¨ï¼šä¿æŒä¸»é¢˜ä¸€è‡´ã€ä¿ç•™æ ¸å¿ƒæ„æ€ï¼Œåœ¨å…¶åŸºç¡€ä¸Šæ¶¦è‰²ä¼˜åŒ–å³å¯
2) å¦‚å‡ºç°è‹±æ–‡è¯ï¼šç¿»è¯‘ä¸ºä¸­æ–‡ï¼ˆä¸“æœ‰åè¯å¯ä¿ç•™åŸæ–‡å¹¶åŠ ä¸­æ–‡é‡Šä¹‰ï¼‰
3) tags è¾“å‡º 1-4 ä¸ªï¼Œå»é‡ï¼Œä¸è¦å¸¦ # ç¬¦å·ï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨åŠ  #ï¼‰
4) title/description/tags å…¨éƒ¨ç”¨ä¸­æ–‡è¡¨è¾¾ä¸ºä¸»ï¼Œä¸è¦è¡¨æƒ…ç¬¦å·
"""

                # è°ƒç”¨AIæ¨¡å‹
                response = await model_manager.call_current_model(
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=500
                )

                if response["status"] != "success":
                    raise Exception(response.get("error", "AIè°ƒç”¨å¤±è´¥"))

                # è§£æAIè¿”å›çš„JSON
                import json
                import re

                content = response["content"]
                # å°è¯•æå–JSON
                json_match = re.search(r'\{[^}]+\}', content, re.DOTALL)
                if json_match:
                    metadata = json.loads(json_match.group())
                else:
                    metadata = json.loads(content)

                ai_title = str(metadata.get("title", "") or "").strip()
                ai_description = str(metadata.get("description", "") or "").strip()
                raw_tags = metadata.get("tags", [])
                if isinstance(raw_tags, str):
                    # tolerate accidental "tag1 tag2" output
                    raw_tags = [t for t in re.split(r"[\s,ï¼Œ]+", raw_tags) if t and t.strip()]
                ai_tags = []
                if isinstance(raw_tags, list):
                    for t in raw_tags:
                        s = str(t).strip()
                        if not s:
                            continue
                        s = s.lstrip("#").strip()
                        if s and s not in ai_tags:
                            ai_tags.append(s)
                ai_tags = ai_tags[:4]

                # ä¿å­˜åˆ°æ•°æ®åº“
                cursor.execute("""
                    UPDATE file_records
                    SET ai_title = ?, ai_description = ?, ai_tags = ?, ai_generated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                """, (ai_title, ai_description, json.dumps(ai_tags, ensure_ascii=False), file_id))
                db.commit()

                results.append({
                    "file_id": file_id,
                    "status": "success",
                    "ai_title": ai_title,
                    "ai_description": ai_description,
                    "ai_tags": ai_tags
                })
                success_count += 1

                logger.info(f"AI metadata generated for file {file_id}: {ai_title}")

            except Exception as e:
                logger.error(f"Failed to generate AI metadata for file {file_id}: {e}")
                results.append({
                    "file_id": file_id,
                    "status": "failed",
                    "error": str(e)
                })
                failed_count += 1

        return AIMetadataGenerateResponse(
            success_count=success_count,
            failed_count=failed_count,
            results=results
        )

    except Exception as e:
        logger.error(f"Batch generate AI metadata error: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ç”Ÿæˆå¤±è´¥: {str(e)}")


# ============================================
# AIè¯­éŸ³è½¬æ–‡å­— (Whisper)
# ============================================

@router.post(
    "/transcribe-audio",
    response_model=TranscribeAudioResponse,
    summary="è§†é¢‘è¯­éŸ³è½¬æ–‡å­—",
    description="""
    ä½¿ç”¨Whisperæ¨¡å‹å°†è§†é¢‘ä¸­çš„è¯­éŸ³è½¬æ¢ä¸ºæ–‡å­—ã€‚

    åŠŸèƒ½ï¼š
    - æ”¯æŒå¤šç§è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡ç­‰ï¼‰
    - è‡ªåŠ¨æå–è§†é¢‘éŸ³é¢‘
    - é«˜ç²¾åº¦è¯­éŸ³è¯†åˆ«
    - ä¿å­˜è½¬å½•æ–‡æœ¬ç”¨äºç”Ÿæˆæ›´ä¸°å¯Œçš„æ ‡é¢˜å’Œæè¿°

    å‚æ•°ï¼š
    - file_id: è§†é¢‘æ–‡ä»¶ID
    - language: è¯­è¨€ä»£ç ï¼ˆzh, enç­‰ï¼‰
    - model: Whisperæ¨¡å‹åç§°
    """
)
async def transcribe_audio(
    request: TranscribeAudioRequest,
    db=Depends(get_db)
):
    """è§†é¢‘è¯­éŸ³è½¬æ–‡å­—"""
    try:
        cursor = db.cursor()

        # æŸ¥è¯¢æ–‡ä»¶ä¿¡æ¯
        cursor.execute("""
            SELECT id, filename, file_path, duration
            FROM file_records
            WHERE id = ?
        """, (request.file_id,))

        row = cursor.fetchone()
        if not row:
            raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: ID {request.file_id}")

        file_id, filename, file_path, duration = row

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(file_path).exists():
            raise NotFoundException(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # ä½¿ç”¨OpenAI Whisper APIè¿›è¡Œè½¬å½•
        # è¿™é‡Œéœ€è¦æå–éŸ³é¢‘å¹¶è°ƒç”¨Whisper
        from ai_service.model_manager import get_model_manager
        import tempfile
        import subprocess

        # æå–éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as temp_audio:
            temp_audio_path = temp_audio.name

        try:
            # ä½¿ç”¨ffmpegæå–éŸ³é¢‘
            logger.info(f"Extracting audio from {file_path} to {temp_audio_path}")
            subprocess.run([
                "ffmpeg", "-i", file_path,
                "-vn", "-acodec", "libmp3lame",
                "-q:a", "2", temp_audio_path,
                "-y"
            ], check=True, capture_output=True)

            # è°ƒç”¨Whisper API
            model_manager = get_model_manager()

            # ä½¿ç”¨å½“å‰é…ç½®çš„AIæä¾›å•†è°ƒç”¨Whisper
            # æ³¨æ„ï¼šéœ€è¦æ”¯æŒWhisperçš„æä¾›å•†ï¼ˆå¦‚OpenAIå…¼å®¹æ¥å£ï¼‰
            from openai import OpenAI

            # è·å–å½“å‰é…ç½®çš„APIå¯†é’¥å’ŒåŸºç¡€URL
            import json
            config_file = Path(__file__).parent.parent.parent.parent / "ai_service" / "config.json"
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)

            current_provider = config.get("current_provider", "siliconflow")
            provider_config = config["providers"].get(current_provider, {})

            client = OpenAI(
                api_key=provider_config.get("api_key"),
                base_url=provider_config.get("base_url")
            )

            # è¯»å–éŸ³é¢‘æ–‡ä»¶å¹¶è½¬å½•
            with open(temp_audio_path, "rb") as audio_file:
                transcript_response = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: client.audio.transcriptions.create(
                        model=request.model,
                        file=audio_file,
                        language=request.language
                    )
                )

            transcript_text = transcript_response.text

            # ä¿å­˜è½¬å½•æ–‡æœ¬åˆ°æ•°æ®åº“ï¼ˆå¯ä»¥æ·»åŠ æ–°å­—æ®µå­˜å‚¨ï¼‰
            # è¿™é‡Œæˆ‘ä»¬å°†è½¬å½•æ–‡æœ¬ä¿å­˜åˆ°æè¿°å­—æ®µæˆ–å•ç‹¬çš„å­—æ®µ
            logger.info(f"Transcription completed for file {file_id}: {len(transcript_text)} characters")

            # è·å–è§†é¢‘æ—¶é•¿
            if not duration:
                # ä½¿ç”¨ffprobeè·å–æ—¶é•¿
                result = subprocess.run([
                    "ffprobe", "-v", "error",
                    "-show_entries", "format=duration",
                    "-of", "default=noprint_wrappers=1:nokey=1",
                    file_path
                ], capture_output=True, text=True, check=True)
                duration = float(result.stdout.strip())

                # æ›´æ–°æ•°æ®åº“
                cursor.execute("UPDATE file_records SET duration = ? WHERE id = ?", (duration, file_id))
                db.commit()

            return TranscribeAudioResponse(
                file_id=file_id,
                transcript=transcript_text,
                language=request.language,
                duration=duration or 0.0
            )

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if Path(temp_audio_path).exists():
                Path(temp_audio_path).unlink()

    except NotFoundException:
        raise
    except Exception as e:
        logger.error(f"Transcribe audio error: {e}")
        raise HTTPException(status_code=500, detail=f"è½¬å½•å¤±è´¥: {str(e)}")

